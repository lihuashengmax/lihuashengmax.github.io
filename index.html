<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="技而无术，知而上进">
<meta property="og:type" content="website">
<meta property="og:title" content="李桦笙的博客">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="李桦笙的博客">
<meta property="og:description" content="技而无术，知而上进">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="李桦笙">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>李桦笙的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="李桦笙的博客" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">李桦笙的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">分享与记录，学习与生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E5%BC%80%E7%AF%87%E9%9A%8F%E6%83%B3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E5%BC%80%E7%AF%87%E9%9A%8F%E6%83%B3/" class="post-title-link" itemprop="url">随想</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-13 08:57:57" itemprop="dateModified" datetime="2022-08-13T08:57:57+08:00">2022-08-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9A%8F%E6%83%B3/" itemprop="url" rel="index"><span itemprop="name">随想</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>&emsp;&emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>&emsp;&emsp;人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>&emsp;&emsp;作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>&emsp;&emsp;而博客能够完全满足该类需求。<br>&emsp;&emsp;以上。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/TED%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/TED%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/" class="post-title-link" itemprop="url">随想</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-22 20:31:21" itemprop="dateModified" datetime="2022-08-22T20:31:21+08:00">2022-08-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TED%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">TED笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>而博客能够完全满足该类需求。<br>以上。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>表头</th>
<th>表头</th>
</tr>
</thead>
<tbody>
<tr>
<td>单元格</td>
<td>单元格</td>
</tr>
<tr>
<td>单元格</td>
<td>单元格</td>
</tr>
</tbody>
</table>
</div>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E6%91%84%E5%BD%B1%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E6%91%84%E5%BD%B1%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/" class="post-title-link" itemprop="url">随想</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-13 09:14:02" itemprop="dateModified" datetime="2022-08-13T09:14:02+08:00">2022-08-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%91%84%E5%BD%B1%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">摄影笔记</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>&emsp;&emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>&emsp;&emsp;人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>&emsp;&emsp;作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>&emsp;&emsp;而博客能够完全满足该类需求。<br>&emsp;&emsp;以上。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%83/" class="post-title-link" itemprop="url">神经网络随记之七：关于缓解神经网络训练过拟合的方法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-03 14:42:45" itemprop="dateModified" datetime="2022-09-03T14:42:45+08:00">2022-09-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="关于缓解神经网络训练过拟合的方法"><a href="#关于缓解神经网络训练过拟合的方法" class="headerlink" title="关于缓解神经网络训练过拟合的方法"></a>关于缓解神经网络训练过拟合的方法</h2><h3 id="正则化（regularization）的技术之一：权重衰减（weight-decay）"><a href="#正则化（regularization）的技术之一：权重衰减（weight-decay）" class="headerlink" title="正则化（regularization）的技术之一：权重衰减（weight decay）"></a>正则化（regularization）的技术之一：权重衰减（weight decay）</h3><p>&emsp;&emsp;首先明确$L_1$与$L_2$范数的定义：对于矩阵$\mathbf{X}\in\mathbb{R}^{m\times n}$，其$L_1$范数为$\Vert\mathbf{x}\Vert_1=\sum^{m}_{i=1}\sum^{n}_{j=1}\vert x_{ij}\vert$，其$L_2$范数为$\Vert\mathbf{x}\Vert_2=\sqrt{\sum^{m}_{i=1}\sum^{n}_{j=1}x^2_{ij}}$。</p>
<p>&emsp;&emsp;在训练参数化机器学习模型时，权重衰减（weight decay）是最⼴泛使⽤的正则化的技术之⼀，其通常也被称为$L_2$正则化，具体操作为在神经网络原有的损失函数基础上增添权重矩阵的$L_2$范数项，即$loss_{new}=loss_{old}+\sum\frac{2}{\lambda}\Vert\mathbf{w}\Vert^2$。</p>
<h3 id="正则化（regularization）的技术之二：暂退法（dropout）"><a href="#正则化（regularization）的技术之二：暂退法（dropout）" class="headerlink" title="正则化（regularization）的技术之二：暂退法（dropout）"></a>正则化（regularization）的技术之二：暂退法（dropout）</h3><p>&emsp;&emsp;暂退法的使用相当于在神经网络中新增添了一个dropout层（通常加在激活函数层后），该层对接收到的特征向量$\mathbf{x}$进行暂退处理，即：</p>
<script type="math/tex; mode=display">
x_i=\left\{
\begin{array}{l}
0\ \ \ \ \ \ \ \ \ \ \ 概率为p\\
\frac{x_i}{1-p} \ \ \ \ \ \ \ 其它情况  
\end{array}
\right.</script><p>以保证期望不变，其中$p$值由$dropout$值决定，一般来说，dropout层仅在训练网络时使用，而在测试网络时不发挥其功能。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E7%94%B5%E9%9F%B3%E4%B8%8E%E7%BC%96%E6%9B%B2%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E7%94%B5%E9%9F%B3%E4%B8%8E%E7%BC%96%E6%9B%B2%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/" class="post-title-link" itemprop="url">随想</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-13 09:13:40" itemprop="dateModified" datetime="2022-08-13T09:13:40+08:00">2022-08-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%94%B5%E9%9F%B3%E4%B8%8E%E7%BC%96%E6%9B%B2/" itemprop="url" rel="index"><span itemprop="name">电音与编曲</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>&emsp;&emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>&emsp;&emsp;人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>&emsp;&emsp;作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>&emsp;&emsp;而博客能够完全满足该类需求。<br>&emsp;&emsp;以上。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%89/" class="post-title-link" itemprop="url">神经网络随记之三：关于softmax函数中的数值问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-02 14:50:54" itemprop="dateModified" datetime="2022-09-02T14:50:54+08:00">2022-09-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="关于softmax函数中的数值问题"><a href="#关于softmax函数中的数值问题" class="headerlink" title="关于softmax函数中的数值问题"></a>关于softmax函数中的数值问题</h2><h3 id="指数计算带来的数值稳定性问题"><a href="#指数计算带来的数值稳定性问题" class="headerlink" title="指数计算带来的数值稳定性问题"></a>指数计算带来的数值稳定性问题</h3><p>&emsp;&emsp;回顾激活函数的公式：$softmax(x_i)=\frac{exp(x_i)}{\sum exp(x_j)}=\frac{exp(x_i-max(x_j))}{\sum exp(x_j-max(x_j))}$，由于$x_i$由神经网络最后一层的全连接层输出，其值可能非常大，从而$exp(x_i)$的值可能非常大；$x_i-max(x_j)$的值可能非常小，从而$exp(x_i-max(x_j))$的值可能非常接近于$0$，因此需要找到方法解决这一因为指数的引入而带来的数值稳定性问题。</p>
<h3 id="将softmax计算与交叉熵计算相结合解决数值稳定性问题"><a href="#将softmax计算与交叉熵计算相结合解决数值稳定性问题" class="headerlink" title="将softmax计算与交叉熵计算相结合解决数值稳定性问题"></a>将softmax计算与交叉熵计算相结合解决数值稳定性问题</h3><p>&emsp;&emsp;将$softmax$函数与交叉熵函数计算结合在一起，有$log(softmax(x_i))=x_i-max(x_j)-log(\sum(x_j-max(x_j)))$，从而解决上述的数值稳定性问题。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%BA%94/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%BA%94/" class="post-title-link" itemprop="url">神经网络随记之五：关于pytorch中的迭代器与生成器</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-02 18:44:03" itemprop="dateModified" datetime="2022-09-02T18:44:03+08:00">2022-09-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="关于pytorch中的迭代器与生成器"><a href="#关于pytorch中的迭代器与生成器" class="headerlink" title="关于pytorch中的迭代器与生成器"></a>关于pytorch中的迭代器与生成器</h2><h3 id="什么是迭代器与生成器"><a href="#什么是迭代器与生成器" class="headerlink" title="什么是迭代器与生成器"></a>什么是迭代器与生成器</h3><h3 id="pytorch中的迭代器与生成器"><a href="#pytorch中的迭代器与生成器" class="headerlink" title="pytorch中的迭代器与生成器"></a>pytorch中的迭代器与生成器</h3>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E5%9B%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E5%9B%9B/" class="post-title-link" itemprop="url">神经网络随记之四：关于多层感知机中的激活函数</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-09-02 18:43:55" itemprop="dateModified" datetime="2022-09-02T18:43:55+08:00">2022-09-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="关于多层感知机中的激活函数"><a href="#关于多层感知机中的激活函数" class="headerlink" title="关于多层感知机中的激活函数"></a>关于多层感知机中的激活函数</h2><p>&emsp;&emsp;多层感知机MLP（multilayer perception）在隐藏层中加入激活函数以实现神经网络的非线性。</p>
<h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>&emsp;&emsp;$ReLU(x)=max(x,0)$，图像如下：<image src="../pictures/ReLU.png"></p>
<p>&emsp;&emsp;其导数图像如下：<image src="../pictures/ReLU导数.png"></p>
<p>&emsp;&emsp;$ReLU$函数的一个变体函数$pReLU(x)=max(0,x)+\alpha min(0,x)$。</p>
<h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>&emsp;&emsp;$sigmoid(x)=\frac{1}{1+exp(-x)}$，图像如下：<image src="../pictures/sigmoid.png"></p>
<p>&emsp;&emsp;其导数可记为$\frac{d}{dx}sigmoid(x)=sigmoid(x)(1-sigmoid(x))$，图像如下：<image src="../pictures/sigmoid导数.png"></p>
<h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>&emsp;&emsp;$tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}$，图像如下：<image src="../pictures/tanh.png"></p>
<p>&emsp;&emsp;其导数可记为$\frac{d}{dx}tanh(x)=1-tanh^2(x)$，图像如下：<image src="../pictures/tanh导数.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%80/" class="post-title-link" itemprop="url">神经网络随记之一：关于线性回归的解析解</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-30 19:06:42" itemprop="dateModified" datetime="2022-08-30T19:06:42+08:00">2022-08-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="关于线性回归的解析解"><a href="#关于线性回归的解析解" class="headerlink" title="关于线性回归的解析解"></a>关于线性回归的解析解</h2><h3 id="原线性模型及其变式"><a href="#原线性模型及其变式" class="headerlink" title="原线性模型及其变式"></a>原线性模型及其变式</h3><p>&emsp;&emsp;设$\mathbf{X}\in\mathbb{R}^{n\times d}$为$n$个特征向量，其中$d$为特征个数，$\mathbf{w}\in\mathbb{R}^d$为一$d\times 1$的权重向量，则线性模型的预测结果$\hat{\mathbf{y}}\in\mathbb{R}^n$可表示为$\hat{\mathbf{y}}=\mathbf{Xw}+b$，$b$为偏置标量，在这里随着向量维度进行广播。</p>
<p>&emsp;&emsp;为了进一步更加简洁地表示上述线性模型，可将偏置$b$合并到权重向量$\mathbf{w}$中，具体做法为在输入数据的特征矩阵$\mathbf{X}$的右侧增添一$n$行$1$列的全一向量，变为</p>
<script type="math/tex; mode=display">\mathbf{X}=\begin{bmatrix}
x_{11}&{\cdots}&{x_{1d}}&{1}\\
{\vdots}&{\ddots}&{\vdots}&{\vdots}\\
x_{n1}&{\cdots}&{x_{nd}}&{1}\\
\end{bmatrix}</script><p>在列向量$\mathbf{w}$末尾增添偏置标量$b$，变为</p>
<script type="math/tex; mode=display">\mathbf{w}=\begin{bmatrix}
w_{1}\\
{\vdots}\\
w_{d}\\
b\\
\end{bmatrix}</script><p>则原线性模型可简化为$\hat{\mathbf{y}}=\mathbf{Xw}$。</p>
<h3 id="线性模型的解析解"><a href="#线性模型的解析解" class="headerlink" title="线性模型的解析解"></a>线性模型的解析解</h3><p>&emsp;&emsp;上述线性模型的损失函数由$L_2$范数表示，为$loss=\Vert\mathbf{y}-\mathbf{Xw}\Vert$，需要将损失关于$\mathbf{w}$的导数设为$0$以得到解析解。</p>
<p>&emsp;&emsp;通过引入二次型对损失函数进行改写，$loss=(\mathbf{y}-\mathbf{Xw})^TE(\mathbf{y}-\mathbf{Xw})$，则$\frac{\partial loss}{\partial\mathbf{w}}=\frac{\partial(\mathbf{y}-\mathbf{Xw})^TE(\mathbf{y}-\mathbf{Xw})}{\partial(\mathbf{y}-\mathbf{Xw})}\frac{\partial(\mathbf{y}-\mathbf{Xw})}{\partial\mathbf{w}}=-2\mathbf{X}^T(\mathbf{y}-\mathbf{Xw})$，令$\frac{\partial loss}{\partial\mathbf{w}}=0$，则有$\mathbf{X}^T\mathbf{y}=\mathbf{X}^T\mathbf{Xw}$，解得$\mathbf{w}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty$，即为该线性模型的解析解。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/08/12/%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/gk.jpeg">
      <meta itemprop="name" content="李桦笙">
      <meta itemprop="description" content="技而无术，知而上进">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="李桦笙的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/08/12/%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA/" class="post-title-link" itemprop="url">如何利用pytorch搭建简单神经网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-08-12 00:00:00" itemprop="dateCreated datePublished" datetime="2022-08-12T00:00:00+08:00">2022-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-08-26 11:05:32" itemprop="dateModified" datetime="2022-08-26T11:05:32+08:00">2022-08-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="使用pytorch搭建神经网络步骤详解"><a href="#使用pytorch搭建神经网络步骤详解" class="headerlink" title="使用pytorch搭建神经网络步骤详解"></a>使用pytorch搭建神经网络步骤详解</h2><p>&emsp;&emsp;在做图像识别相关项目时，整个流程可分为“准备数据集”、“设计模型”、“构建损失函数与优化器”、“进行训练与测试”四个步骤，pytorch提供了丰富的工具能够高效迅速地完成以上四个子任务。</p>
<h3 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h3><p>&emsp;&emsp;首先需要明确数据集构建过程中的几个重要名词。</p>
<ul>
<li><p><strong>epoch</strong>：一个epoch的完成表明训练数据集中的所有数据均已通过了一次网络模型。</p>
</li>
<li><p><strong>batch-size</strong>：由于网络模型规模受限，实际训练时不可能一次性将训练数据集中所有数据都通过网络，因此需要将数据集中的数据分为一个个batch，其中batch的size通常为2的倍数。</p>
</li>
<li><p><strong>iteration</strong>：表明迭代次数，一个迭代的完成表明一个batch的数据集已经通过了模型。</p>
</li>
</ul>
<p>&emsp;&emsp;数据集构建的代码模板总结如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset <span class="comment">#util是utiliy的缩写，意为实用工具包</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>): <span class="comment">#pytorch已经提供了抽象类Dataset，在其基础上构建子类进行继承，其中除了_init_外，_getitem_与_len_函数必须重写</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_getitem_</span>(<span class="params">self,index</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_len_</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">dataset=MyDataset()</span><br><span class="line"><span class="comment">#train_loader=DataLoader(dataset=dataset,batch_size=32,shuffle=True,num_workers=2) </span></span><br><span class="line"><span class="comment">#test_loader=DataLoader(dataset=dataset,batch_size=32,shuffle=True,num_workers=2) #num_workers表征了将指定序号的batch加载到RAM中的速度</span></span><br></pre></td></tr></table></figure>
<h3 id="设计模型"><a href="#设计模型" class="headerlink" title="设计模型"></a>设计模型</h3><p>&emsp;&emsp;pytorch提供了父类torch.nn.Module，通过继承该类可以构建不同的网络结构，设计模型的代码模板总结如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet,self)._init_()</span><br><span class="line">        <span class="comment">#构建网络结构的每一层</span></span><br><span class="line">        <span class="comment">#self.layer1=torch.nn.***   每层结构</span></span><br><span class="line">        <span class="comment">#self.activate=torch.nn.*** 激活函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#构建网络层间的激活函数关系</span></span><br><span class="line">        <span class="comment">#x=self.activate(self.layer1(x))</span></span><br><span class="line">        <span class="comment">#***</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net=MyNet()</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;TIPS：关于torch.nn与torch.nn.functional的异同可总结为下表。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">torch.nn.***</th>
<th style="text-align:center">torch.nn.functional.***</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">是类或者对象</td>
<td style="text-align:center">是函数</td>
</tr>
<tr>
<td style="text-align:center">类在每次实例化时内部参数都会进行初始化，因此类的实例化应放入_init_（只运行一次），每次对对象的调用应放入forward（多次运行），一般将层的实例化放入_init_，将实例化对象的调用放入forward</td>
<td style="text-align:center">一般在该函数外部定义并初始化相应参数比如在_init_，在forward中传入该参数运行函数</td>
</tr>
</tbody>
</table>
</div>
<p>&emsp;&emsp;对于纯文本数字形式的数据集，网络结构以全连接层为主；对于图像形式的数据集，网络结构以全连接层与卷积层为主，具体的代码形式及意义如下：</p>
<ul>
<li><strong>全连接层（又称线性层）</strong>：torch.nn.Linear(in_features,out_features,bias=True)</li>
</ul>
<p>&emsp;&emsp;其对传入该层的数据进行线性变换$y=xW+b$，其中$x$为一$batch\_ size$行$in\_ features$列矩阵，$in\_ features$为线性层输入数据的特征数；$W$为一$in\_ features$行$out\_ features$列矩阵，决定了该线性层输出数据的特征数；$b$为一$batch\_ size$行$out\_ features$列矩阵，$out\_ features$为线性层输出数据的特征数。</p>
<ul>
<li><strong>卷积层（二维）</strong>：torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,<br>dilation=1,groups=1,bias=True,padding_mode=’zeros’,device=None)</li>
</ul>
<p>&emsp;&emsp;其通过卷积核与传入的图像数据进行“矩阵内积”的计算以进行特征提取，$in\_ channels$等于卷积层传入$feature\_ map$的通道数，$out\_ channels$等于卷积层传出$feature\_ map$的通道数，此时该卷积层共有$out\_ channels$个卷积核，每个卷积核的维度为$in\_ channels*kernal\_ size$，$stride$为卷积核的移动步长，$padding$为对传入$feature\_ map$周围填充的圈数，整个卷积层传入$feature\_ map$的维度为$batch\_ size* in\_ channels* in\_ height* in\_ width$，传出$feature\_ map$的维度为$batch\_ size* out\_ channels* out\_ height* out\_ width$。</p>
<ul>
<li><strong>最大池化层（二维）</strong>：torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)</li>
</ul>
<p>&emsp;&emsp;参数中$stride$默认等于$kernel\_ size$。</p>
<p>&emsp;&emsp;TIPS：在卷积神经网络中，在卷积层的输出与全连接层的输入之间需要进行张量维度间的转换，例如对于张量<script type="math/tex">x</script>，通过$x.view(batch\_ size,-1)$将其进行“平铺”。</p>
<h3 id="构建损失函数与优化器"><a href="#构建损失函数与优化器" class="headerlink" title="构建损失函数与优化器"></a>构建损失函数与优化器</h3><p>&emsp;&emsp;构建损失函数与优化器的代码模板总结如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">criterion=torch.nn.***</span><br><span class="line">optimizer=optim.***</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;在此对一些基本的损失函数及其计算方式加以说明，其中损失函数不作均值处理：</p>
<ul>
<li><p><strong>MSELoss（Mean Square Error Loss）</strong>:均方损失函数，$loss=\sum(y_i-\hat{y})^2$，多用于回归问题。</p>
</li>
<li><p><strong>CrossEntropyLoss</strong>：交叉熵损失函数，其输入为神经网络最后一层全连接层的输出$y$，CrossEntropyLoss首先将其通过softmax激活函数，其公式为$softmax(y_i)=\frac{exp(y_i)}{\sum exp(y_j)}$得到一组向量$\hat{y}$；然后将网络预测结果取对数，其公式为$\hat{Y}_i=log\hat{y}_i$；再将$\hat{Y}$与真值标签独热向量$Y$进行损失计算，其公式为$loss=\sum-Y_i\hat{Y}_i$。</p>
</li>
<li><p><strong>NLLLoss（Negative Log Likelihood Loss）</strong>：负对数似然损失函数，进行过程对应于交叉熵损失函数进行过程的第三部分。</p>
</li>
<li><p><strong>BCELoss（Binary Cross Entropy Loss）</strong>：二分类交叉熵损失函数，为交叉熵损失函数在分类结果是二分类时的特殊情况（值得注意的是，为简化，神经网络在二分类问题的输出结果为一数值标量而非向量，标签也不再采用独热向量的形式，而是以数值$0$与$1$进行类别区分），其输入为神经网络最后一层全连接层的输出$y$通过sigmoid激活函数的结果，其公式为$sigmoid(y)=\frac{1}{1+e^{-y}}$，即所得到数值标量$\hat{y}$；由于分类结果只有两类，因此损失值可以直接计算为$loss=-(ylog\hat{y}+(1-y)log(1-\hat{y}))$。</p>
</li>
</ul>
<p>&emsp;&emsp;在此对一些基本优化器所采用的梯度下降方法及其计算方式加以说明：</p>
<ul>
<li><p><strong>BGD（Batch Gradient Descent）</strong>：批量梯度下降法，在整个训练数据集通过神经网络后根据损失值进行参数优化。</p>
</li>
<li><p><strong>SGD（Stochastic Gradient Descent）</strong>：随机梯度下降法，在训练集中的每个数据通过神经网络后都要根据损失值进行参数优化，只有该梯度下降方法在pytorch中可以直接调用，其它两种梯度下降方法可以间接实现，其需要设置的参数主要为模型的参数（net.parameters()）、学习率（lr）、动量系数（momentum），学习率与负梯度相乘，动量系数与上一次的参数更新相乘，两个乘积与现参数相加以完成更新。</p>
</li>
<li><p><strong>MBGD（Mini-batch Gradient Descent）</strong>：小批量梯度下降法，在训练集中每个$batch$的数据通过神经网络后根据损失值进行参数优化。</p>
</li>
</ul>
<h3 id="进行训练与测试"><a href="#进行训练与测试" class="headerlink" title="进行训练与测试"></a>进行训练与测试</h3><p>&emsp;&emsp;神经网络模型训练的代码模板总结如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader,<span class="number">0</span>): <span class="comment">#从下标0开始给train_loader编号，每一个for循环进行一个batch的训练</span></span><br><span class="line">        inputs,target=data <span class="comment">#inpus与target为一个batch的数据</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#将optimizer中储存的梯度清零，否则在进行loss.backward()时会将反向传播的梯度与原储存的梯度相加得到错误结果</span></span><br><span class="line">        <span class="comment">#前向传播+反向传播+参数更新</span></span><br><span class="line">        outputs=net(inputs) <span class="comment">#前向传播</span></span><br><span class="line">        loss=criterion(outputs,target) <span class="comment">#计算损失值</span></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#参数更新</span></span><br><span class="line">        <span class="comment">#前向传播+反向传播+参数更新</span></span><br><span class="line">        running_loss+=loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx%batch_num==(batch_num-<span class="number">1</span>): <span class="comment">#每batch_num个batch打印一次running_loss</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d,%5d] loss:%.3f&#x27;</span>%(epoch+<span class="number">1</span>,batch_idx+<span class="number">1</span>,running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss=<span class="number">0.0</span></span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;神经网络模型测试的代码模板总结如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct=<span class="number">0</span></span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            inputs,labels=data</span><br><span class="line">            outputs=net(inputs)</span><br><span class="line">            _,predicted=torch.<span class="built_in">max</span>(outputs.data,dim=<span class="number">1</span>) <span class="comment">#dim=0表示按列求最大值，并返回最大值的索引；dim=1表示按行求最大值，并返回最大值的索引</span></span><br><span class="line">            total+=labels.size(<span class="number">0</span>) <span class="comment">#标签个数</span></span><br><span class="line">            correct+=(predicted==labels).<span class="built_in">sum</span>().item() <span class="comment">#&#x27;==&#x27;运算符将张量元素逐个比较，返回布尔类型填充的相应张量；.item只能用于只含一个标量的张量中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set:%d %%&#x27;</span>%(<span class="number">100</span>*correct/total))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="李桦笙"
      src="/images/gk.jpeg">
  <p class="site-author-name" itemprop="name">李桦笙</p>
  <div class="site-description" itemprop="description">技而无术，知而上进</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李桦笙</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
