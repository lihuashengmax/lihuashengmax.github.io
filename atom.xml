<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>李桦笙的博客</title>
  
  <subtitle>分享与记录，学习与生活</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-08-13T00:57:57.990Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>李桦笙</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>随想</title>
    <link href="http://example.com/2022/08/12/%E5%BC%80%E7%AF%87%E9%9A%8F%E6%83%B3/"/>
    <id>http://example.com/2022/08/12/%E5%BC%80%E7%AF%87%E9%9A%8F%E6%83%B3/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-08-13T00:57:57.990Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>&emsp;&emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>&emsp;&emsp;人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>&emsp;&emsp;作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>&emsp;&emsp;而博客能够完全满足该类需求。<br>&emsp;&emsp;以上。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BLOG开篇简辞&quot;&gt;&lt;a href=&quot;#BLOG开篇简辞&quot; class=&quot;headerlink&quot; title=&quot;BLOG开篇简辞&quot;&gt;&lt;/a&gt;BLOG开篇简辞&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取</summary>
      
    
    
    
    <category term="随想" scheme="http://example.com/categories/%E9%9A%8F%E6%83%B3/"/>
    
    
  </entry>
  
  <entry>
    <title>随想</title>
    <link href="http://example.com/2022/08/12/TED%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/"/>
    <id>http://example.com/2022/08/12/TED%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-08-22T12:31:21.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>而博客能够完全满足该类需求。<br>以上。</p><div class="table-container"><table><thead><tr><th>表头</th><th>表头</th></tr></thead><tbody><tr><td>单元格</td><td>单元格</td></tr><tr><td>单元格</td><td>单元格</td></tr></tbody></table></div>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BLOG开篇简辞&quot;&gt;&lt;a href=&quot;#BLOG开篇简辞&quot; class=&quot;headerlink&quot; title=&quot;BLOG开篇简辞&quot;&gt;&lt;/a&gt;BLOG开篇简辞&lt;/h2&gt;&lt;p&gt;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样</summary>
      
    
    
    
    <category term="TED笔记" scheme="http://example.com/categories/TED%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>随想</title>
    <link href="http://example.com/2022/08/12/%E6%91%84%E5%BD%B1%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/"/>
    <id>http://example.com/2022/08/12/%E6%91%84%E5%BD%B1%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-08-13T01:14:02.386Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>&emsp;&emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>&emsp;&emsp;人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>&emsp;&emsp;作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>&emsp;&emsp;而博客能够完全满足该类需求。<br>&emsp;&emsp;以上。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BLOG开篇简辞&quot;&gt;&lt;a href=&quot;#BLOG开篇简辞&quot; class=&quot;headerlink&quot; title=&quot;BLOG开篇简辞&quot;&gt;&lt;/a&gt;BLOG开篇简辞&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取</summary>
      
    
    
    
    <category term="摄影笔记" scheme="http://example.com/categories/%E6%91%84%E5%BD%B1%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络随记之七：关于缓解神经网络训练过拟合的方法</title>
    <link href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%83/"/>
    <id>http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%83/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-09-03T06:42:45.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于缓解神经网络训练过拟合的方法"><a href="#关于缓解神经网络训练过拟合的方法" class="headerlink" title="关于缓解神经网络训练过拟合的方法"></a>关于缓解神经网络训练过拟合的方法</h2><h3 id="正则化（regularization）的技术之一：权重衰减（weight-decay）"><a href="#正则化（regularization）的技术之一：权重衰减（weight-decay）" class="headerlink" title="正则化（regularization）的技术之一：权重衰减（weight decay）"></a>正则化（regularization）的技术之一：权重衰减（weight decay）</h3><p>&emsp;&emsp;首先明确$L_1$与$L_2$范数的定义：对于矩阵$\mathbf{X}\in\mathbb{R}^{m\times n}$，其$L_1$范数为$\Vert\mathbf{x}\Vert_1=\sum^{m}_{i=1}\sum^{n}_{j=1}\vert x_{ij}\vert$，其$L_2$范数为$\Vert\mathbf{x}\Vert_2=\sqrt{\sum^{m}_{i=1}\sum^{n}_{j=1}x^2_{ij}}$。</p><p>&emsp;&emsp;在训练参数化机器学习模型时，权重衰减（weight decay）是最⼴泛使⽤的正则化的技术之⼀，其通常也被称为$L_2$正则化，具体操作为在神经网络原有的损失函数基础上增添权重矩阵的$L_2$范数项，即$loss_{new}=loss_{old}+\sum\frac{2}{\lambda}\Vert\mathbf{w}\Vert^2$。</p><h3 id="正则化（regularization）的技术之二：暂退法（dropout）"><a href="#正则化（regularization）的技术之二：暂退法（dropout）" class="headerlink" title="正则化（regularization）的技术之二：暂退法（dropout）"></a>正则化（regularization）的技术之二：暂退法（dropout）</h3><p>&emsp;&emsp;暂退法的使用相当于在神经网络中新增添了一个dropout层（通常加在激活函数层后），该层对接收到的特征向量$\mathbf{x}$进行暂退处理，即：</p><script type="math/tex; mode=display">x_i=\left\{\begin{array}{l}0\ \ \ \ \ \ \ \ \ \ \ 概率为p\\\frac{x_i}{1-p} \ \ \ \ \ \ \ 其它情况  \end{array}\right.</script><p>以保证期望不变，其中$p$值由$dropout$值决定，一般来说，dropout层仅在训练网络时使用，而在测试网络时不发挥其功能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关于缓解神经网络训练过拟合的方法&quot;&gt;&lt;a href=&quot;#关于缓解神经网络训练过拟合的方法&quot; class=&quot;headerlink&quot; title=&quot;关于缓解神经网络训练过拟合的方法&quot;&gt;&lt;/a&gt;关于缓解神经网络训练过拟合的方法&lt;/h2&gt;&lt;h3 id=&quot;正则化（regul</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>随想</title>
    <link href="http://example.com/2022/08/12/%E7%94%B5%E9%9F%B3%E4%B8%8E%E7%BC%96%E6%9B%B2%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/"/>
    <id>http://example.com/2022/08/12/%E7%94%B5%E9%9F%B3%E4%B8%8E%E7%BC%96%E6%9B%B2%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%80/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-08-13T01:13:40.882Z</updated>
    
    <content type="html"><![CDATA[<h2 id="BLOG开篇简辞"><a href="#BLOG开篇简辞" class="headerlink" title="BLOG开篇简辞"></a>BLOG开篇简辞</h2><p>&emsp;&emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取知识或者信息，大脑在这样的行为中会逐渐削弱自身的独立思考与判断能力。<br>&emsp;&emsp;人们不再愿意静下心来好好花上一阵子时间来阅读与反思，思想会变得肤浅且丧失主见。<br>&emsp;&emsp;作为上述情况的补救，通过文字的撰写而锻炼自己的大脑是一种低成本且易施行的方法，通过大量的文章将自己的知识体系与随笔感想进行梳理与回顾，修正大脑的思维方式，将信息系统化与深刻化。<br>&emsp;&emsp;而博客能够完全满足该类需求。<br>&emsp;&emsp;以上。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;BLOG开篇简辞&quot;&gt;&lt;a href=&quot;#BLOG开篇简辞&quot; class=&quot;headerlink&quot; title=&quot;BLOG开篇简辞&quot;&gt;&lt;/a&gt;BLOG开篇简辞&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;在这个信息碎片化的互联网时代，人们往往更热衷于通过较为快捷的方式获取</summary>
      
    
    
    
    <category term="电音与编曲" scheme="http://example.com/categories/%E7%94%B5%E9%9F%B3%E4%B8%8E%E7%BC%96%E6%9B%B2/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络随记之三：关于softmax函数中的数值问题</title>
    <link href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%89/"/>
    <id>http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%89/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-09-02T06:50:54.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于softmax函数中的数值问题"><a href="#关于softmax函数中的数值问题" class="headerlink" title="关于softmax函数中的数值问题"></a>关于softmax函数中的数值问题</h2><h3 id="指数计算带来的数值稳定性问题"><a href="#指数计算带来的数值稳定性问题" class="headerlink" title="指数计算带来的数值稳定性问题"></a>指数计算带来的数值稳定性问题</h3><p>&emsp;&emsp;回顾激活函数的公式：$softmax(x_i)=\frac{exp(x_i)}{\sum exp(x_j)}=\frac{exp(x_i-max(x_j))}{\sum exp(x_j-max(x_j))}$，由于$x_i$由神经网络最后一层的全连接层输出，其值可能非常大，从而$exp(x_i)$的值可能非常大；$x_i-max(x_j)$的值可能非常小，从而$exp(x_i-max(x_j))$的值可能非常接近于$0$，因此需要找到方法解决这一因为指数的引入而带来的数值稳定性问题。</p><h3 id="将softmax计算与交叉熵计算相结合解决数值稳定性问题"><a href="#将softmax计算与交叉熵计算相结合解决数值稳定性问题" class="headerlink" title="将softmax计算与交叉熵计算相结合解决数值稳定性问题"></a>将softmax计算与交叉熵计算相结合解决数值稳定性问题</h3><p>&emsp;&emsp;将$softmax$函数与交叉熵函数计算结合在一起，有$log(softmax(x_i))=x_i-max(x_j)-log(\sum(x_j-max(x_j)))$，从而解决上述的数值稳定性问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关于softmax函数中的数值问题&quot;&gt;&lt;a href=&quot;#关于softmax函数中的数值问题&quot; class=&quot;headerlink&quot; title=&quot;关于softmax函数中的数值问题&quot;&gt;&lt;/a&gt;关于softmax函数中的数值问题&lt;/h2&gt;&lt;h3 id=&quot;指数计算带</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络随记之五：关于pytorch中的迭代器与生成器</title>
    <link href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%BA%94/"/>
    <id>http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%BA%94/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-09-02T10:44:03.810Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于pytorch中的迭代器与生成器"><a href="#关于pytorch中的迭代器与生成器" class="headerlink" title="关于pytorch中的迭代器与生成器"></a>关于pytorch中的迭代器与生成器</h2><h3 id="什么是迭代器与生成器"><a href="#什么是迭代器与生成器" class="headerlink" title="什么是迭代器与生成器"></a>什么是迭代器与生成器</h3><h3 id="pytorch中的迭代器与生成器"><a href="#pytorch中的迭代器与生成器" class="headerlink" title="pytorch中的迭代器与生成器"></a>pytorch中的迭代器与生成器</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关于pytorch中的迭代器与生成器&quot;&gt;&lt;a href=&quot;#关于pytorch中的迭代器与生成器&quot; class=&quot;headerlink&quot; title=&quot;关于pytorch中的迭代器与生成器&quot;&gt;&lt;/a&gt;关于pytorch中的迭代器与生成器&lt;/h2&gt;&lt;h3 id=&quot;什</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络随记之四：关于多层感知机中的激活函数</title>
    <link href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E5%9B%9B/"/>
    <id>http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E5%9B%9B/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-09-02T10:43:55.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于多层感知机中的激活函数"><a href="#关于多层感知机中的激活函数" class="headerlink" title="关于多层感知机中的激活函数"></a>关于多层感知机中的激活函数</h2><p>&emsp;&emsp;多层感知机MLP（multilayer perception）在隐藏层中加入激活函数以实现神经网络的非线性。</p><h3 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h3><p>&emsp;&emsp;$ReLU(x)=max(x,0)$，图像如下：<image src="../pictures/ReLU.png"></p><p>&emsp;&emsp;其导数图像如下：<image src="../pictures/ReLU导数.png"></p><p>&emsp;&emsp;$ReLU$函数的一个变体函数$pReLU(x)=max(0,x)+\alpha min(0,x)$。</p><h3 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h3><p>&emsp;&emsp;$sigmoid(x)=\frac{1}{1+exp(-x)}$，图像如下：<image src="../pictures/sigmoid.png"></p><p>&emsp;&emsp;其导数可记为$\frac{d}{dx}sigmoid(x)=sigmoid(x)(1-sigmoid(x))$，图像如下：<image src="../pictures/sigmoid导数.png"></p><h3 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h3><p>&emsp;&emsp;$tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}$，图像如下：<image src="../pictures/tanh.png"></p><p>&emsp;&emsp;其导数可记为$\frac{d}{dx}tanh(x)=1-tanh^2(x)$，图像如下：<image src="../pictures/tanh导数.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关于多层感知机中的激活函数&quot;&gt;&lt;a href=&quot;#关于多层感知机中的激活函数&quot; class=&quot;headerlink&quot; title=&quot;关于多层感知机中的激活函数&quot;&gt;&lt;/a&gt;关于多层感知机中的激活函数&lt;/h2&gt;&lt;p&gt;&amp;emsp;&amp;emsp;多层感知机MLP（multi</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络随记之一：关于线性回归的解析解</title>
    <link href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%80/"/>
    <id>http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%B8%80/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-08-30T11:06:42.002Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于线性回归的解析解"><a href="#关于线性回归的解析解" class="headerlink" title="关于线性回归的解析解"></a>关于线性回归的解析解</h2><h3 id="原线性模型及其变式"><a href="#原线性模型及其变式" class="headerlink" title="原线性模型及其变式"></a>原线性模型及其变式</h3><p>&emsp;&emsp;设$\mathbf{X}\in\mathbb{R}^{n\times d}$为$n$个特征向量，其中$d$为特征个数，$\mathbf{w}\in\mathbb{R}^d$为一$d\times 1$的权重向量，则线性模型的预测结果$\hat{\mathbf{y}}\in\mathbb{R}^n$可表示为$\hat{\mathbf{y}}=\mathbf{Xw}+b$，$b$为偏置标量，在这里随着向量维度进行广播。</p><p>&emsp;&emsp;为了进一步更加简洁地表示上述线性模型，可将偏置$b$合并到权重向量$\mathbf{w}$中，具体做法为在输入数据的特征矩阵$\mathbf{X}$的右侧增添一$n$行$1$列的全一向量，变为</p><script type="math/tex; mode=display">\mathbf{X}=\begin{bmatrix}x_{11}&{\cdots}&{x_{1d}}&{1}\\{\vdots}&{\ddots}&{\vdots}&{\vdots}\\x_{n1}&{\cdots}&{x_{nd}}&{1}\\\end{bmatrix}</script><p>在列向量$\mathbf{w}$末尾增添偏置标量$b$，变为</p><script type="math/tex; mode=display">\mathbf{w}=\begin{bmatrix}w_{1}\\{\vdots}\\w_{d}\\b\\\end{bmatrix}</script><p>则原线性模型可简化为$\hat{\mathbf{y}}=\mathbf{Xw}$。</p><h3 id="线性模型的解析解"><a href="#线性模型的解析解" class="headerlink" title="线性模型的解析解"></a>线性模型的解析解</h3><p>&emsp;&emsp;上述线性模型的损失函数由$L_2$范数表示，为$loss=\Vert\mathbf{y}-\mathbf{Xw}\Vert$，需要将损失关于$\mathbf{w}$的导数设为$0$以得到解析解。</p><p>&emsp;&emsp;通过引入二次型对损失函数进行改写，$loss=(\mathbf{y}-\mathbf{Xw})^TE(\mathbf{y}-\mathbf{Xw})$，则$\frac{\partial loss}{\partial\mathbf{w}}=\frac{\partial(\mathbf{y}-\mathbf{Xw})^TE(\mathbf{y}-\mathbf{Xw})}{\partial(\mathbf{y}-\mathbf{Xw})}\frac{\partial(\mathbf{y}-\mathbf{Xw})}{\partial\mathbf{w}}=-2\mathbf{X}^T(\mathbf{y}-\mathbf{Xw})$，令$\frac{\partial loss}{\partial\mathbf{w}}=0$，则有$\mathbf{X}^T\mathbf{y}=\mathbf{X}^T\mathbf{Xw}$，解得$\mathbf{w}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^Ty$，即为该线性模型的解析解。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关于线性回归的解析解&quot;&gt;&lt;a href=&quot;#关于线性回归的解析解&quot; class=&quot;headerlink&quot; title=&quot;关于线性回归的解析解&quot;&gt;&lt;/a&gt;关于线性回归的解析解&lt;/h2&gt;&lt;h3 id=&quot;原线性模型及其变式&quot;&gt;&lt;a href=&quot;#原线性模型及其变式&quot; c</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>如何利用pytorch搭建简单神经网络</title>
    <link href="http://example.com/2022/08/12/%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA/"/>
    <id>http://example.com/2022/08/12/%E7%AE%80%E5%8D%95%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%90%AD%E5%BB%BA/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-08-26T03:05:32.702Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用pytorch搭建神经网络步骤详解"><a href="#使用pytorch搭建神经网络步骤详解" class="headerlink" title="使用pytorch搭建神经网络步骤详解"></a>使用pytorch搭建神经网络步骤详解</h2><p>&emsp;&emsp;在做图像识别相关项目时，整个流程可分为“准备数据集”、“设计模型”、“构建损失函数与优化器”、“进行训练与测试”四个步骤，pytorch提供了丰富的工具能够高效迅速地完成以上四个子任务。</p><h3 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h3><p>&emsp;&emsp;首先需要明确数据集构建过程中的几个重要名词。</p><ul><li><p><strong>epoch</strong>：一个epoch的完成表明训练数据集中的所有数据均已通过了一次网络模型。</p></li><li><p><strong>batch-size</strong>：由于网络模型规模受限，实际训练时不可能一次性将训练数据集中所有数据都通过网络，因此需要将数据集中的数据分为一个个batch，其中batch的size通常为2的倍数。</p></li><li><p><strong>iteration</strong>：表明迭代次数，一个迭代的完成表明一个batch的数据集已经通过了模型。</p></li></ul><p>&emsp;&emsp;数据集构建的代码模板总结如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset <span class="comment">#util是utiliy的缩写，意为实用工具包</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>): <span class="comment">#pytorch已经提供了抽象类Dataset，在其基础上构建子类进行继承，其中除了_init_外，_getitem_与_len_函数必须重写</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_getitem_</span>(<span class="params">self,index</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_len_</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">dataset=MyDataset()</span><br><span class="line"><span class="comment">#train_loader=DataLoader(dataset=dataset,batch_size=32,shuffle=True,num_workers=2) </span></span><br><span class="line"><span class="comment">#test_loader=DataLoader(dataset=dataset,batch_size=32,shuffle=True,num_workers=2) #num_workers表征了将指定序号的batch加载到RAM中的速度</span></span><br></pre></td></tr></table></figure><h3 id="设计模型"><a href="#设计模型" class="headerlink" title="设计模型"></a>设计模型</h3><p>&emsp;&emsp;pytorch提供了父类torch.nn.Module，通过继承该类可以构建不同的网络结构，设计模型的代码模板总结如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNet,self)._init_()</span><br><span class="line">        <span class="comment">#构建网络结构的每一层</span></span><br><span class="line">        <span class="comment">#self.layer1=torch.nn.***   每层结构</span></span><br><span class="line">        <span class="comment">#self.activate=torch.nn.*** 激活函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        <span class="comment">#构建网络层间的激活函数关系</span></span><br><span class="line">        <span class="comment">#x=self.activate(self.layer1(x))</span></span><br><span class="line">        <span class="comment">#***</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">net=MyNet()</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;TIPS：关于torch.nn与torch.nn.functional的异同可总结为下表。</p><div class="table-container"><table><thead><tr><th style="text-align:center">torch.nn.***</th><th style="text-align:center">torch.nn.functional.***</th></tr></thead><tbody><tr><td style="text-align:center">是类或者对象</td><td style="text-align:center">是函数</td></tr><tr><td style="text-align:center">类在每次实例化时内部参数都会进行初始化，因此类的实例化应放入_init_（只运行一次），每次对对象的调用应放入forward（多次运行），一般将层的实例化放入_init_，将实例化对象的调用放入forward</td><td style="text-align:center">一般在该函数外部定义并初始化相应参数比如在_init_，在forward中传入该参数运行函数</td></tr></tbody></table></div><p>&emsp;&emsp;对于纯文本数字形式的数据集，网络结构以全连接层为主；对于图像形式的数据集，网络结构以全连接层与卷积层为主，具体的代码形式及意义如下：</p><ul><li><strong>全连接层（又称线性层）</strong>：torch.nn.Linear(in_features,out_features,bias=True)</li></ul><p>&emsp;&emsp;其对传入该层的数据进行线性变换$y=xW+b$，其中$x$为一$batch\_ size$行$in\_ features$列矩阵，$in\_ features$为线性层输入数据的特征数；$W$为一$in\_ features$行$out\_ features$列矩阵，决定了该线性层输出数据的特征数；$b$为一$batch\_ size$行$out\_ features$列矩阵，$out\_ features$为线性层输出数据的特征数。</p><ul><li><strong>卷积层（二维）</strong>：torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,<br>dilation=1,groups=1,bias=True,padding_mode=’zeros’,device=None)</li></ul><p>&emsp;&emsp;其通过卷积核与传入的图像数据进行“矩阵内积”的计算以进行特征提取，$in\_ channels$等于卷积层传入$feature\_ map$的通道数，$out\_ channels$等于卷积层传出$feature\_ map$的通道数，此时该卷积层共有$out\_ channels$个卷积核，每个卷积核的维度为$in\_ channels*kernal\_ size$，$stride$为卷积核的移动步长，$padding$为对传入$feature\_ map$周围填充的圈数，整个卷积层传入$feature\_ map$的维度为$batch\_ size* in\_ channels* in\_ height* in\_ width$，传出$feature\_ map$的维度为$batch\_ size* out\_ channels* out\_ height* out\_ width$。</p><ul><li><strong>最大池化层（二维）</strong>：torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)</li></ul><p>&emsp;&emsp;参数中$stride$默认等于$kernel\_ size$。</p><p>&emsp;&emsp;TIPS：在卷积神经网络中，在卷积层的输出与全连接层的输入之间需要进行张量维度间的转换，例如对于张量<script type="math/tex">x</script>，通过$x.view(batch\_ size,-1)$将其进行“平铺”。</p><h3 id="构建损失函数与优化器"><a href="#构建损失函数与优化器" class="headerlink" title="构建损失函数与优化器"></a>构建损失函数与优化器</h3><p>&emsp;&emsp;构建损失函数与优化器的代码模板总结如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line">criterion=torch.nn.***</span><br><span class="line">optimizer=optim.***</span><br></pre></td></tr></table></figure><p>&emsp;&emsp;在此对一些基本的损失函数及其计算方式加以说明，其中损失函数不作均值处理：</p><ul><li><p><strong>MSELoss（Mean Square Error Loss）</strong>:均方损失函数，$loss=\sum(y_i-\hat{y})^2$，多用于回归问题。</p></li><li><p><strong>CrossEntropyLoss</strong>：交叉熵损失函数，其输入为神经网络最后一层全连接层的输出$y$，CrossEntropyLoss首先将其通过softmax激活函数，其公式为$softmax(y_i)=\frac{exp(y_i)}{\sum exp(y_j)}$得到一组向量$\hat{y}$；然后将网络预测结果取对数，其公式为$\hat{Y}_i=log\hat{y}_i$；再将$\hat{Y}$与真值标签独热向量$Y$进行损失计算，其公式为$loss=\sum-Y_i\hat{Y}_i$。</p></li><li><p><strong>NLLLoss（Negative Log Likelihood Loss）</strong>：负对数似然损失函数，进行过程对应于交叉熵损失函数进行过程的第三部分。</p></li><li><p><strong>BCELoss（Binary Cross Entropy Loss）</strong>：二分类交叉熵损失函数，为交叉熵损失函数在分类结果是二分类时的特殊情况（值得注意的是，为简化，神经网络在二分类问题的输出结果为一数值标量而非向量，标签也不再采用独热向量的形式，而是以数值$0$与$1$进行类别区分），其输入为神经网络最后一层全连接层的输出$y$通过sigmoid激活函数的结果，其公式为$sigmoid(y)=\frac{1}{1+e^{-y}}$，即所得到数值标量$\hat{y}$；由于分类结果只有两类，因此损失值可以直接计算为$loss=-(ylog\hat{y}+(1-y)log(1-\hat{y}))$。</p></li></ul><p>&emsp;&emsp;在此对一些基本优化器所采用的梯度下降方法及其计算方式加以说明：</p><ul><li><p><strong>BGD（Batch Gradient Descent）</strong>：批量梯度下降法，在整个训练数据集通过神经网络后根据损失值进行参数优化。</p></li><li><p><strong>SGD（Stochastic Gradient Descent）</strong>：随机梯度下降法，在训练集中的每个数据通过神经网络后都要根据损失值进行参数优化，只有该梯度下降方法在pytorch中可以直接调用，其它两种梯度下降方法可以间接实现，其需要设置的参数主要为模型的参数（net.parameters()）、学习率（lr）、动量系数（momentum），学习率与负梯度相乘，动量系数与上一次的参数更新相乘，两个乘积与现参数相加以完成更新。</p></li><li><p><strong>MBGD（Mini-batch Gradient Descent）</strong>：小批量梯度下降法，在训练集中每个$batch$的数据通过神经网络后根据损失值进行参数优化。</p></li></ul><h3 id="进行训练与测试"><a href="#进行训练与测试" class="headerlink" title="进行训练与测试"></a>进行训练与测试</h3><p>&emsp;&emsp;神经网络模型训练的代码模板总结如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader,<span class="number">0</span>): <span class="comment">#从下标0开始给train_loader编号，每一个for循环进行一个batch的训练</span></span><br><span class="line">        inputs,target=data <span class="comment">#inpus与target为一个batch的数据</span></span><br><span class="line">        optimizer.zero_grad() <span class="comment">#将optimizer中储存的梯度清零，否则在进行loss.backward()时会将反向传播的梯度与原储存的梯度相加得到错误结果</span></span><br><span class="line">        <span class="comment">#前向传播+反向传播+参数更新</span></span><br><span class="line">        outputs=net(inputs) <span class="comment">#前向传播</span></span><br><span class="line">        loss=criterion(outputs,target) <span class="comment">#计算损失值</span></span><br><span class="line">        loss.backward() <span class="comment">#反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#参数更新</span></span><br><span class="line">        <span class="comment">#前向传播+反向传播+参数更新</span></span><br><span class="line">        running_loss+=loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx%batch_num==(batch_num-<span class="number">1</span>): <span class="comment">#每batch_num个batch打印一次running_loss</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d,%5d] loss:%.3f&#x27;</span>%(epoch+<span class="number">1</span>,batch_idx+<span class="number">1</span>,running_loss / <span class="number">300</span>))</span><br><span class="line">            running_loss=<span class="number">0.0</span></span><br></pre></td></tr></table></figure><p>&emsp;&emsp;神经网络模型测试的代码模板总结如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct=<span class="number">0</span></span><br><span class="line">    total=<span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            inputs,labels=data</span><br><span class="line">            outputs=net(inputs)</span><br><span class="line">            _,predicted=torch.<span class="built_in">max</span>(outputs.data,dim=<span class="number">1</span>) <span class="comment">#dim=0表示按列求最大值，并返回最大值的索引；dim=1表示按行求最大值，并返回最大值的索引</span></span><br><span class="line">            total+=labels.size(<span class="number">0</span>) <span class="comment">#标签个数</span></span><br><span class="line">            correct+=(predicted==labels).<span class="built_in">sum</span>().item() <span class="comment">#&#x27;==&#x27;运算符将张量元素逐个比较，返回布尔类型填充的相应张量；.item只能用于只含一个标量的张量中</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Accuracy on test set:%d %%&#x27;</span>%(<span class="number">100</span>*correct/total))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;使用pytorch搭建神经网络步骤详解&quot;&gt;&lt;a href=&quot;#使用pytorch搭建神经网络步骤详解&quot; class=&quot;headerlink&quot; title=&quot;使用pytorch搭建神经网络步骤详解&quot;&gt;&lt;/a&gt;使用pytorch搭建神经网络步骤详解&lt;/h2&gt;&lt;p&gt;&amp;e</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络随记之六：关于一些名词</title>
    <link href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E5%85%AD/"/>
    <id>http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E5%85%AD/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-09-04T04:49:43.447Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于一些名词"><a href="#关于一些名词" class="headerlink" title="关于一些名词"></a>关于一些名词</h2><ul><li><p><strong>训练误差（training error）</strong>：模型在训练数据集上计算得到的误差。</p></li><li><p><strong>泛化误差（training error）</strong>：模型应⽤在同样从原始样本的分布中抽取的⽆限多数据样本时，模型误差的期望，而非在测试集或者验证集上的误差。</p></li><li><p><strong>K折交叉验证（K-Folder Cross Validation）</strong>：原始训练数据被分成K个不重叠的⼦集，然后执⾏K次模型训练和验证：每次在K−1个⼦集上进⾏训练，并在剩余的⼀个⼦集（在该轮中没有⽤于训练的⼦集）上进⾏验证，最后通过对K次实验的结果取平均来估计训练和验证误差。</p></li><li><p><strong>特征映射（feature map）</strong>：即为卷积层，可被看作是⼀个输⼊映射到下⼀层的空间维度的转换器。</p></li><li><p><strong>感受野（receptive field）</strong>：在卷积神经⽹络中，对于某⼀层的任意元素x，在前向传播期间可能影响x计算的所有元素（来⾃所有先前层）。</p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关于一些名词&quot;&gt;&lt;a href=&quot;#关于一些名词&quot; class=&quot;headerlink&quot; title=&quot;关于一些名词&quot;&gt;&lt;/a&gt;关于一些名词&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;训练误差（training error）&lt;/strong&gt;：模型在训练数据</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络随记之一：关于极大似然估计</title>
    <link href="http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%BA%8C/"/>
    <id>http://example.com/2022/08/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9A%8F%E8%AE%B0%E4%B9%8B%E4%BA%8C/</id>
    <published>2022-08-11T16:00:00.000Z</published>
    <updated>2022-08-30T11:09:01.123Z</updated>
    
    <content type="html"><![CDATA[<h2 id="关于极大似然估计"><a href="#关于极大似然估计" class="headerlink" title="关于极大似然估计"></a>关于极大似然估计</h2><h3 id="极大似然估计与线性回归模型所采用的均方误差损失函数"><a href="#极大似然估计与线性回归模型所采用的均方误差损失函数" class="headerlink" title="极大似然估计与线性回归模型所采用的均方误差损失函数"></a>极大似然估计与线性回归模型所采用的均方误差损失函数</h3><h3 id="极大似然估计与神经网络分类所采用的交叉熵损失函数"><a href="#极大似然估计与神经网络分类所采用的交叉熵损失函数" class="headerlink" title="极大似然估计与神经网络分类所采用的交叉熵损失函数"></a>极大似然估计与神经网络分类所采用的交叉熵损失函数</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;关于极大似然估计&quot;&gt;&lt;a href=&quot;#关于极大似然估计&quot; class=&quot;headerlink&quot; title=&quot;关于极大似然估计&quot;&gt;&lt;/a&gt;关于极大似然估计&lt;/h2&gt;&lt;h3 id=&quot;极大似然估计与线性回归模型所采用的均方误差损失函数&quot;&gt;&lt;a href=&quot;#极大似然</summary>
      
    
    
    
    <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
</feed>
